# 微服务

## 分布式

### SOA架构

​		Service Oriented Architecture，面向服务的架构，是一种设计方法，其中包含多个服务，服务之间通过相互依赖最终提供一系列的功能。一个服务通常以独立的形式存在于操作系统进程中，各个服务之间通过网络调用。注重系统集成方面。

### 分布式架构

​		分布式是从物理资源的角度去将不同的机器组成一个整体对外服务，为了解决单个物理服务器容量和性能瓶颈问题而采用的优化手段。微服务关注的是完全分离，彻底的服务化、组件化。

#### 优缺点

​		优点：每个服务足够内聚，足够小、开发效率提高；服务之间可以独立部署；每个服务可以各自进行x扩展和z扩展；每个服务可以根据自己的需要部署到合适的硬件服务器上；一个服务的内存泄露并不会让整个系统瘫痪。

​		缺点：需处理分布式系统的复杂性，如服务之间的通信机制、分布式事务；在生产环境中要管理多个不同的服务的实例，

#### 水平扩展

​		当一台机器扛不住流量时，就通过添加机器的方式，将流量平分到所有服务器上，所有机器都可以提供相当的服务；

#### 垂直拆分

​		前端有多种查询需求时，一台机器扛不住，可以将不同的需求分发到不同的机器上，比如A机器处理余票查询的请求，B机器处理支付的请求。

#### 通信方式

​		若两个子系统运行在同个操作系统的两个进程中，没有在网络上进行分离，则其通信方式额外有：共享内存、信号量、文件系统、内核消息队列、管道等，本质上都是通过操作系统内核机制来进行数据和消息的交互而无须经过网络协议栈。

### CAP

**一致性 Consistency**

**可用性 Available**

**分区容错性 Partition Tolerance**：即分布式系统在遇到某节点或网络分区故障的时候，仍然能够对外提供满足一致性或可用性的服务。

一个分布式系统最多只能同时满足其中的两项，P是必须的，因此往往选择就在CP或者AP中。

### RPC

​		RPC，Remote Procedure Call，远程过程调用。是一个计算机通信协议。该协议允许运行于一台计算机的程序调用另一台计算机的子程序，而程序员无需额外地为这个交互作用编程。对于Java这类的面向对象编程语言，远程过程调用亦可称作远程调用或远程方法调用。HTTP、TCP、Socket实际上就是一种RCP协议。

#### RPC框架

​		提供一种透明调用机制让使用者不必显式的区分本地调用和远程调用。RPC服务方通过 RpcServer 去导出（export）远程接口方法，而客户方通过 RpcClient 去引入（import）远程接口方法。客户方像调用本地方法一样去调用远程接口方法。

#### 调用过程

1. RPC 框架提供接口的代理实现，实际的调用将委托给代理RpcProxy。 代理封装方法、参数等调用信息组装成能够进行网络传输的消息体，并将调用转交给RpcInvoker去实际执行。 在客户端的RpcInvoker通过连接器RpcConnector去维持与服务端的通道RpcChannel，并使用RpcProtocol执行协议编码（encode）并将编码后的请求消息通过通道发送给服务方。

2. RPC 服务端接收器RpcAcceptor接收客户端的调用请求，同样使用RpcProtocol执行协议解码decode。 解码后的调用信息传递给 RpcProcessor 去控制处理调用过程，最后再委托调用给RpcInvoker去实际执行并将返回结果打包成消息并发送至消费方。

### 多线程

​		多线程是指从软件或者硬件上实现多个线程并发执行的技术，解决CPU调度多个进程的问题，从而让这些进程看上去是同时执行（实际是交替运行的），使CPU调度能力最大化。

### 高并发

​		高并发是从业务角度去描述系统的能力，实现高并发的手段可以通过分布式技术去解决，将并发流量分不到不同的物理服务器上；使用缓存系统，将所有的，静态内容放到CDN等；使用多线程技术将一台服务器的服务能力最大化。

### 分布式锁

​		单机环境下，使用Synchronized关键字修饰方法或代码块，来保证其中的代码同一时间只能被一个线程执行。Synchronized是JVM提供的关键字，只能在一台JVM中保证同一时间只有一个线程执行，到了分布式环境中，若部署了多个节点，则需用分布式锁来保证多个节点在同一时间只有一个线程执行该代码。

#### 实现

1. 基于数据库：数据库创建一个表，表中包含方法名等字段，在方法名上创建唯一索引，想要执行某个方法，需使用该方法名向表中插入数据，成功插入则获取锁，执行完后删除对应的行数据释放锁。性能差、易死锁、不阻塞、不可重入。

2. 基于redis

3. 基于zookeeper

### 分布式事务

#### 刚性事务

##### 二阶段提交

​		二阶段提交把事务分为两个阶段，并引入事务管理器。缺点是严重依赖于数据库层面来搞定复杂的事务，效率很低。

第一阶段：

1. 事务管理器向所有资源管理器（数据库）发送事务内容prepare，询问是否可以执行事务提交操作，等待各参与者响应。
2. 各参与者执行事务操作，将undo和redo信息记入事务日志。
3. 各参与者向事务管理器反馈事务询问的响应，若事务操作执行成功，则返回yes响应表示事务可以执行；若执行失败则返回no。

第二阶段：

1. 若事务管理器收到的响应均为yes，则执行事务提交，向所有参与者发出commit请求。若收到了no响应或等待操作，则进入事务中断，向所有参与者发送rollback请求，请求资源回滚，并释放资源。
2. 参与者收到commit请求后，正式执行事务提交操作，提交完成后释放整个事务执行期间占用的事务资源，并向事务管理器发送ack消息表示已提交。
3. 事务管理器接收到所有参与者反馈的ack消息后，完成事务。

##### 三阶段提交 

#### 柔性事务

##### 补偿事务

###### TCC

​		TCC全称为Try、Confirm、Cancel，基于二阶段提交。缺点是事务回滚严重依赖于自己写代码来回滚和补偿，会造成补偿代码巨大。

Try阶段：对各个服务的资源做检测以及对资源进行锁定或者预留。

Confirm阶段：在各个服务中执行实际的操作。

Cancel阶段：如果任何一个服务的业务方法执行出错，那么这里就需要进行补偿，把那些执行成功的回滚。

##### 消息型事务

###### 本地消息表

​		A系统在自己本地一个事务里操作同时，插入一条数据到消息表；接着 A 系统将这个消息发送到 MQ 中去；B 系统接收到消息之后，在一个事务里，往自己本地消息表里插入一条数据，同时执行其他的业务操作，如果这个消息已经被处理过了，那么此时这个事务会回滚，这样保证不会重复处理消息；B 系统执行成功之后，就会更新自己本地消息表的状态以及 A 系统消息表的状态；如果 B 系统处理失败了，那么就不会更新消息表状态，那么此时 A 系统会定时扫描自己的消息表，如果有未处理的消息，会再次发送到 MQ 中去，让 B 再次处理；这个方案保证了最终一致性，哪怕 B 事务失败了，但是 A 会不断重发消息，直到 B 那边成功为止。这个方案说实话最大的问题就在于严重依赖于数据库的消息表来管理事务。

###### MQ事务型消息

### 缓存/熔断/降级

#### 缓存

​		目的是提升系统访问速度，增大系统处理容量。

#### 熔断

​		当下游服务因访问压力过大而响应变慢或失败，上游服务为了保护系统整体的可用性，可以暂时切断对下游服务的调用。牺牲局部，保全整体。

#### 降级

​		服务降级是从整个系统的负荷情况出发和考虑的，对某些负荷会比较高的情况，为了预防某些功能出现负荷过载或者响应慢的情况，暂时舍弃对一些非核心的接口和数据的请求，直接返回一个提前准备好的fallback错误处理信息。虽然提供的是一个有损的服务，但却保证了整个系统的稳定性和可用性，待高峰后或问题解决后再开放服务。

**熔断和降级的区别**：触发原因不同，服务熔断一般是某个下游服务故障引起，而服务降级一般是从整体负荷考虑。服务熔断与降级实现框架：Hystrix

### 流量削峰

​		更多地延缓用户请求，以及层层过滤用户的访问需求，遵从“最后落地到数据库的请求数要尽量少”的原则。对于秒杀这样的高并发场景业务，最基本的原则就是将请求拦截在系统上游，降低下游压力。如果不在前端拦截很可能造成数据库(mysql、oracle等)读写锁冲突，甚至导致死锁，最终还有可能出现雪崩等场景。

#### 消息队列

​		用消息队列来缓冲瞬时流量，把同步的直接调用转换成异步的间接推送，中间通过一个队列在一端承接瞬时的流量洪峰，在另一端平滑地将消息推送出去。

#### 分层过滤

1. 通过在不同的层次尽可能地过滤掉无效请求。
2. 通过CDN过滤掉大量的图片，静态资源的请求。
3. 通过类似Redis这样的分布式缓存，过滤请求等就是典型的在上游拦截读请求。
4. 对写数据进行基于时间的合理分片，过滤掉过期的失效请求。
5. 对写请求做限流保护，将超出系统承载能力的请求过滤掉。
6. 涉及到的读数据不做强一致性校验，减少因为一致性校验产生瓶颈的问题。
7. 对写数据进行强一致性校验，只保留最后有效的数据。

### 限流

​		通过对一个时间窗口内的请求进行限速来保护系统的稳定，一旦超出限制的速率可执行拒绝服务、排队、等待、降级等处理。

#### 实现方式

1. nginx的limit_conn模块限制瞬时并发连接数
2. nginx的limit_req模块限制每秒的平均速率
3. tomcat的线程池参数，限制总并发/连接/请求数。acceptCount为若tomcat的线程均忙于响应，新来的连接则进入队列排队，超出排队大小则拒绝连接；maxConnections为瞬时最大连接数，超出的会排队等待；maxThreads为tomcat所能启动的最大线程数，若请求处理量一直远大于线程数则可能会僵死。
4. 限制总资源数：线程池、连接池等，如限制数据库连接，超出可以等待或抛异常。
5. 限流某个接口：秒杀大闸、信号量、AtomicLong等限制接口总并发量；令牌桶、RateLimiter等平滑限流某个接口的请求数。

#### 限流算法

##### 计数器法

​		设置一个计数器counter，每当一个请求过来的时候，counter就加1，如果 counter的值大于100并且该请求与第一个请求的间隔时间还在 1 分钟之内，那么说明请求数过多；如果该请求与第一个请求的间隔时间大于1分钟，且counter的值还在限流范围内，那就重置counter。缺点：统计的精度太低，无法处理临界问题。如果我在单位时间1s 内的前10ms，已经通过了100个请求，那后面的990ms，只能眼巴巴的把请求拒绝，我们把这种现象称为“突刺现象” 。

##### 滑动窗口法

​		将窗口更加细分，每个窗口都有自己的计数器，当总计算达到限定时，限流。只是将计算法变得更平滑而已。本质一样。

##### 漏斗法

​		将容器比作一个漏斗，当请求进来时，相当于水倒入漏斗，然后从下端小口慢慢匀速的流出。不管上面流量多大，下面流出的速度始终保持不变。弊端是无法应对短时间的突发流量。 

##### 令牌桶算法

​		在令牌桶算法中，存在一个桶，用来存放固定数量的令牌。算法中存在一种机制，以一定的速率往桶中放令牌。每次请求调用需要先获取令牌，只有拿到令牌，才有机会继续执行，否则选择选择等待可用的令牌、或者直接拒绝。 

​		Google开源工具包Guava提供了限流工具类RateLimiter，该类基于令牌桶算法(Token Bucket)来完成限流，非常易于使用。RateLimiter经常用于限制对一些物理资源或者逻辑资源的访问速率，它支持两种获取permits接口，tryAcquire()如果拿不到立刻返回 false（），tryAcquire(long timeout, TimeUnit unit)会阻塞等待一段时间看能不能拿到（）。缺点：整合RateLimiter代码重复量特别大，不支持注解方式。 

#### guava包

##### RateLimiter

​		限流抽象类，定义限流器的基本接口。

##### SmoothRateLimiter

​		平滑限流实现器，也是一个抽象类。

##### SmoothWarmingUp

​		自带预热机制的限流器实现类型。

##### SmoothBursty

​		适应于突发流量的限流器。核心设计思想基本与令牌桶类似，但还是有些不同。

###### 设计思想

1. SmoothBursty 以指定的速率生成许可，在SmoothBursty中用storedPermits表示。
2. 当一个请求需要申请许可时，如果需要申请的许可数小于 storedPermits ，则消耗指定许可，直接返回，无需等待。
3. 当一个请求需要申请的许可大于 storedPermits 时，则计算需要等待的时间，更新下一次许可可发放时间，直接返回，即当请求消耗掉所有许可后，当前请求并不会阻塞，而是影响下一个请求，即支持突发流量。

![image-20200501210602561](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20200501210602561.png)

###### 源码

1. 创建：调用RateLimiter的静态方法create()创建一个限流器

    ```
    static RateLimiter create(SleepingStopwatch stopwatch, double permitsPerSecond){ //1
        RateLimiter rateLimiter = new SmoothBursty(stopwatch, 1.0);//2 
        rateLimiter.setRate(permitsPerSecond);//3
        return rateLimiter;
    }
    ```

    1. 参数stopwatch：秒表，主要是实现当前从启动开始已消耗的时间，有点类似计算一个操作耗时，实现精度纳秒。
        参数dpermitsPerSecond：每秒的许可数，即通常我们说的限流TPS。
    2. 创建SmoothBursty对象
    3. 调用setRate API设置其速率器

2. 构造函数：

    ```
    SmoothBursty(SleepingStopwatch stopwatch,double maxBurstSeconds){
        super(stopwatch);//1
        this.maxBurstSeconds = maxBurstSeconds;
    }
    ```

    1. 为stopWatch与maxBurstSeconds赋值，maxBurstSeconds为允许的突发流量的时间，这里默认为1.0，表示一秒，会影响最大可存储的许可数。

3. setRate()方法：

    ```
    public final void setRate(double permitsPerSecond) {
        checkArgument(
            permitsPerSecond > 0.0 && !Double.isNaN(permitsPerSecond), "rate must be positive");
        synchronized (mutex()) { //1
    		doSetRate(permitsPerSecond, stopwatch.readMicros());//2
        }
    }
    ```

    1. 该方法需要获取该类的监视器，在同步代码块中执行，实现线程安全性。
    2. 调用其具体实现类SmoothRateLimiter的doSetRate方法设置速率。

4. doSetRate()方法：

    ```
    final void doSetRate(double permitsPerSecond,long nowMicros){//1
        resync(nowMicros);//2
        double stableIntervalMicros = SECONDS.toMicros(1L) / permitsPerSecond; //3
        this.stableIntervalMicros = stableIntervalMicros;           
        doSetRate(permitsPerSecond, stableIntervalMicros);//4     
    }
    ```

    1. 参数permitsPerSecond：每秒的许可数，即TPS。
        参数nowMicros：系统已运行时间。 
    2. 基于当前时间重置 SmoothRateLimiter 内部的 storedPermits (已存储的许可数量) 与 nextFreeTicketMicros (下一次可以免费获取许可的时间) 值，所谓的免费指的是无需等待就可以获取设定速率的许可，该方法对理解限流许可的产生非常关键，稍后详细介绍。
    3. 根据 TPS 算出一个稳定的获取1个许可的时间。以一秒发放5个许可，即限速为5TPS，那发放一个许可的世界间隔为 200ms，stableIntervalMicros 变量是以微秒为单位。
    4. 调用 SmoothRateLimiter 的抽象方法 doSetRate 设置速率，这里会调用 SmoothBursty 的 doSetRate 方法。

5. resync()方法：

    ```
    void resync(long nowMicros) {
        if (nowMicros > nextFreeTicketMicros) { //1
    		double newPermits = (nowMicros - nextFreeTicketMicros) / coolDownIntervalMicros();//2
          storedPermits = min(maxPermits, storedPermits + newPermits); //3
          nextFreeTicketMicros = nowMicros; //4
        }
    }
    ```

    1. //如果当前已启动时间大于 nextFreeTicketMicros（下一次可以免费获取许可的时间），则需要重新计算许可，即又可以向许可池中添加许可。
    2. 根据当前时间可增加的许可数量，在 SmoothBursty 的  coolDownIntervalMicros 方法返回的就是上文提到的 stableIntervalMicros (发放一个许可所需要的时间)，故本次可以增加的许可数的算法也好理解，即用当前时间戳减去 nextFreeTicketMicros 的差值，再除以发送一个许可所需要的时间即可。
    3. 计算当前可用的许可。
    4. 更新下一次可增加计算许可的时间。

6. 继续调用doSetRate()方法：

    ```
    void doSetRate(double permitsPerSecond, double stableIntervalMicros) {
        double oldMaxPermits = this.maxPermits;
        maxPermits = maxBurstSeconds * permitsPerSecond;
        if (oldMaxPermits == Double.POSITIVE_INFINITY) {
            storedPermits = maxPermits;
        } else {
            storedPermits =
                (oldMaxPermits == 0.0)
                    ? 0.0 // initial state
                    : storedPermits * maxPermits / oldMaxPermits;
        }
    }
    ```

    1. 初始化 storedPermits 的值，该限速器支持在运行过程中动态改变 permitsPerSecond 的值。

7. RateLimiter.acquire()方法：

    ```
    public double acquire(int permits) {
        long microsToWait = reserve(permits); //1
        stopwatch.sleepMicrosUninterruptibly(microsToWait); //2
        return 1.0 * microsToWait / SECONDS.toMicros(1L); //3
    }
    ```

    1. 根据当前剩余的许可与本次申请的许可来判断本次申请需要等待的时长，如果返回0则表示无需等待。
    2. 如果需要等待的时间不为0，表示触发限速，睡眠指定时间后唤醒。
    3. 返回本次申请等待的时长。

8. reserve()方法：

    ```
    final long reserve(int permits) {
        checkPermits(permits);
        synchronized (mutex()) { //1
          return reserveAndGetWaitLength(permits,stopwatch.readMicros()); //2
        }
    }
    ```

    1. 限速器主要维护的重要数据字段( storedPermits )，对其进行维护时都需要先获取锁。
    2. 调用内部方法 reserveAndGetWaitLength 来计算需要等待时间。

9. reserveAndGetWaitLength()方法：

    ```
    final long reserveAndGetWaitLength(int permits,long nowMicros){
        long momentAvailable = reserveEarliestAvailable(permits, nowMicros);//1
        return max(momentAvailable - nowMicros, 0);//2
    }
    ```

    1. 根据当前拥有的许可数量、当前时间判断待申请许可最早能得到满足的最早时间，用momentAvailable 表示。
    2. 然后计算 momentAvailable 与 nowMicros 的差值与0做比较，得出需要等待的时间。

10. reserveEarliestAvailable()方法：该方法在 RateLimiter 中一个抽象方法，具体实现在其子类 SmoothRateLimiter 中。

    ```
    final long reserveEarliestAvailable(int requiredPermits, long nowMicros) {
        resync(nowMicros); //1
        long returnValue = nextFreeTicketMicros;
        double storedPermitsToSpend = min(requiredPermits, this.storedPermits); //2
        double freshPermits = requiredPermits - storedPermitsToSpend; //3
        long waitMicros =
            storedPermitsToWaitTime(this.storedPermits, storedPermitsToSpend)
                + (long) (freshPermits * stableIntervalMicros); //4
    
        this.nextFreeTicketMicros = LongMath.saturatedAdd(nextFreeTicketMicros, waitMicros); //5
        this.storedPermits -= storedPermitsToSpend; //6
        return returnValue; //7
    }
    ```

    1. 在尝试申请许可之前，先根据当前时间即发放许可速率更新 storedPermits 与 nextFreeTicketMicros（下一次可以免费获取许可的时间）。
    2. 计算本次能从 storedPermits 中消耗的许可数量，取需要申请的许可数量与当前可用的许可数量的最小值，用 storedPermitsToSpend 表示。
    3. 如果需要申请的许可数量( requiredPermits )大于当前剩余许可数量( storedPermits )，则还需要等待新的许可生成，用 freshPermits 表示，即如果该值大于0，则表示本次申请需要阻塞一定时间。
    4. 计算本次申请需要等待的时间，storedPermitsToWaitTime 方法在 SmoothBursty 的实现中默认返回 0，即 SmoothBursty 的等待时间主要来自按照速率生成 freshPermits 个许可的时间，生成一个许可的时间为 stableIntervalMicros，故需要等待的时长为 freshPermits * stableIntervalMicros。
    5. 更新 nextFreeTicketMicros 为当前时间加上需要等待的时间。
    6. 更新 storedPermits 的值，即减少本次已消耗的许可数量。
    7. 这里返回的 returnValue 的值，并没有包含由于剩余许可需要等待创建新许可的时间，即允许一定的突发流量，故本次计算需要的等待时间将对下一次请求生效，这也是框架作者将该限速器取名为 SmoothBursty 的缘由。

## Dubbo

### Dubbo简介

​		阿里开源的一个分布式服务框架，主要用于高性能和透明化的RPC远程服务调用。底层使用Netty这样的NIO框架，基于TCP协议传输，配合Hession 序列化完成RPC通信，性能快。采用全Spring配置方式，透明化接入应用。registry严重依赖第三方组件，当组件出现问题时，服务易中断。

#### 技术选型

​		Spring Cloud是微服务架构的一站式解决方案，功能强大，涵盖面广，比如有服务网关、分布式配置中心、服务跟踪等等。通信基于HTTP的REST方式，相对来说 Http 请求会有更大的报文，占的带宽也会更多，牺牲服务调用的性能。但是 REST 相比 RPC 更为灵活，服务提供方和调用方的依赖只依靠一纸契约，不存在代码级别的强依赖，适合快速演化。

​		当前由于 RPC 协议、注册中心元数据不匹配等问题，在面临微服务基础框架选型时 Dubbo 与 Spring Cloud 只能二选一。现Dubbo 已经适配到 Spring Cloud 生态，比如作为 Spring Cloud 的二进制通信方案来发挥 Dubbo 的性能优势，Dubbo 通过模块化以及对 HTTP 的支持适配到 Spring Cloud。

​		Dubbo起初是个可扩展的RPC调用框架，在Dubbo的一次调用涉及到的服务路由、负载均衡、序列化机制、网络传输协议等等都是可以扩展的，具体的性能取决于所选用的组件。Dubbo的可扩展性是要比Spring Cloud好。在未来的生态中，Dubbo未来会发展成一整套的微服务的解决方案。

```
Dubbo现已捐献给Apache，在阅读官网的源码导读时，发现跟拉下来的最新代码不一样，最近一次更新时间是4月29日。
```

#### 作用

1. 提供高性能的基于代理的远程调用能力，服务以接口为粒度，无任何API侵入，为开发者屏蔽远程调用底层细节。即现在有一个服务，需要提供给别人调用，我们只需向外提供一个接口。其他服务即可像调用本地方法一样调用远程方法，调用过程透明化。

2. 软负载均衡和容错机制。

3. 服务自动注册和发现，不需写死服务提供方的地址，注册中心基于接口名查询服务提供方的IP，且能平滑的增删服务提供者。
    1. 传统用Nginx做负载均衡，需要在Nginx中配置服务器集群的ip，若大促时增加了大量服务器，需要添加很多ip，结束又需要改掉，这是局限。
    2. 服务启动完之后在注册中心中保存一个信息，client可以根据服务名找到所有的服务集群地址，在client端使用一定的负载均衡算法，找到特定的一台服务器发送请求，完成调用。这样每启一个服务就会自动注册到注册中心，client就会监听到注册中心的变化，于是更新负载均衡的列表。

4. 高度可扩展能力。遵循微内核+插件的设计原则，所有核心能力如协议、传输、序列化都被设计为拓展点，平等对待内置实现和第三方实现。

5. 可视化的服务治理与运维。调用时间，调用次数等。

6. 使用Dubbo后：服务调用地址透明、服务返回参数公用、服务Http请求方法透明、对开发者服务调用更透明，开发效率更快。

    ![image-20200430013211169](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20200430013211169.png)

![image-20200430013255209](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20200430013255209.png)

#### 局限

​		集成dubbo时，或多或少需要改一些业务代码。service mesh可以不改业务代码，使得业务代码更加纯粹，单独把负载均衡、路由、容错、降级等调用功能单独提成一个进程，和client绑定在一起。

### 协议支持

**Dubbo**：单一长连接和NIO异步通讯，适合大并发小数据量的服务调用，以及消费者远大于提供者。传输协议 TCP，异步Hessian序列化，性能好。Dubbo框架默认实现了dubbo协议，基于netty实现。也可以使用其他的协议。

```
小数据量大概是100k以内，所以单一消费者无法满足网络带宽，所以dubbo协议适合消费者远大于提供者的场景。dubbo官网有给一张对比图，对于1k大小的POJO，TPS成功平均值最优的情况是dubbo2协议+dubbo序列化+netty，所以Dubbo框架选择dubbo协议作为默认协议。
```

![image-20200501223619347](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20200501223619347.png)

**RMI**：采用JDK标准的RMI协议实现，传输参数和返回参数对象需要实现Serializable接口，使用Java标准序列化机制，使用阻塞式短连接，传输数据包大小混合，消费者和提供者个数差不多，可传文件，传输协议TCP。多个短连接TCP协议传输，同步传输，适用常规的远程服务调用和RMI互操作。在依赖低版本的Common-Collections包，Java序列化存在安全漏洞。

**WebService**：基于WebService的远程调用协议，集成CXF实现，提供和原生WebService的互操作。多个短连接，基于HTTP传输，同步传输，适用系统集成和跨语言调用。

**HTTP**：基于Http表单提交的远程调用协议，使用Spring的HttpInvoke实现。多个短连接同步传输，传输协议HTTP，传入参数大小混合，提供者个数多于消费者，需要给应用程序和浏览器JS调用。

**Hessian**：集成Hessian服务，基于HTTP通讯，采用Servlet暴露服务，Dubbo内嵌Jetty作为服务器时默认实现，提供与Hession服务互操作。多个短连接，同步HTTP传输，Hessian序列化，传入参数较大，提供者大于消费者，提供者压力较大，可传文件。

**Memcache**：基于Memcache实现的RPC协议。

**Redis**：基于Redis实现的RPC协议。

### 架构底层

#### 组件

**Provider**：暴露服务的服务提供方。提供服务接口API、实现服务（实现类）、注册服务（远程注册+本地注册）、暴露服务（启动Tomcat/Netty）。

**Consumer**：调用远程服务的服务消费方。启动时从

**Registry**：服务注册与发现的注册中心，保存服务名与服务地址映射关系。

**Monitor**：统计服务的调用次数与调用时间的监控中心

**Container**：服务运行容器

![image-20200429215953283](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20200429215953283.png)

#### 工作过程

1. 服务器启动、加载、运行服务提供者。
2. provider启动时向注册中心注册自己所提供的服务。
3. consumer启动时向注册中心订阅自己所需的服务。
4. 注册中心返回provider地址列表给consumer，consumer将服务地址缓存。若有变更，注册中心则基于长连接推送变更数据给消费者。
5. consumer从provider地址列表中，基于软负载均衡算法，选一台provider进行调用，若调用失败则选另一台调用。调用需传送接口名、方法名、参数类型列表、参数值列表。
6. consumer和provider在内存中的累计调用次数和调用时间，会每分钟定时发送一次统计数据给监控中心。

![image-20200429215359933](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20200429215359933.png)

### 设计模式

#### 工厂模式

​		provider在export服务时，调用ServiceConfig的export方法，是一种工厂模式，可扩展性强，只需在classpath下增加文件即可扩展，代码零侵入。

```
private static final Protocol protocol = ExtensionLoader.getExtensionLoader(Protocol.class).getAdaptiveExtension();
```

#### 装饰器模式

​		dubbo在启动和调用阶段大量使用了装饰器模式。例如provider提供的调用链代码实在ProtocolFilterWrapper的buildInvokerChain完成的，具体是将注解中含有group=provider的Filter实现，按照order排序，最后调用顺序为EchoFilter->ClassLoaderFilter->GenericFilter->ContextFilter->ExecuteLimitFilter->TraceFilter->TimeoutFilter->MonitorFilter->ExceptionFilter，为装饰器和责任链模式的混合使用。

#### 观察者模式

​		provider启动后将自己的服务注册到注册中心，注册中心订阅时采用了观察者模式，开启的listener定时检查是否有服务更新，若有更新则想provider发送notify消息，provider收到notify消息后，运行NotifyListener的notify方法，执行监听器方法。

#### 动态代理模式

​		Dubbo扩展JDK SPI的类ExtensionLoader的Adaptive实现是典型的动态代理实现。Dubbo 需要灵活地控制实现类，即在调用阶段动态地根据参数决定调用哪个实现类，所以采用先生成代理类的方法，能够做到灵活的调用。生成代理类的代码是ExtensionLoader的createAdaptiveExtensionClassCode方法。代理类主要逻辑是，获取URL参数中指定参数的值作为获取实现类的key。

### 注册发现流程

#### 服务暴露

1. 容器启动，解析配置文件，看是否需要解析配置dubbo。
2. 创建dubbo标签解析器DubboNamespanceHandler。
3. 解析dubbo标签并把配置的标签内容保存起来，保存到ServiceBean里面。
4. serviceBean实现了ApplicationListener<'ContextRefreshedEvent>接口，叫应用的监听器。它监听的事件是ContextRefreshedEvent，当我们IOC容器整个刷新完成，也就是IOC容器里面所有对象都创建完成以后来回调方法onApplicationEvent(ContextRefreshedEvent event)，发布ContextRefreshEvent事件。
5. 监听方法主要是暴露方法的URL地址
6. 分析protocol，得到端口号。
7. 拿到这个url后开启nettyServer服务器，并监听之前得到的端口号。
8. DubboExporter来开启netty服务器，registryExporter用来注册，服务（执行器）和对应的url地址，注册到注册表里。

#### 服务引入

1. Dubbo通过DubboNamespanceHandler解析dubbo:reference/标签，并且Spring会完成Bean的实例化，在引入该Bean的时候，会调用ReferenceBean的getObject方法，以这个方法为入口然后进行服务引入。
2. 服务引入的步骤为：先根据服务名从注册中心找到所有的服务提供者并且封装到服务目录中，然后进行服务路由的初始化，然后进行监听器绑定，然后将服务目录封装为Cluster，最后生成代理类。

#### 服务调用

消费者：

1. Mock机制

2. 服务路由

3. 负载均衡

4. Filter过滤

5. 发送请求

服务者：

1. 接收请求

2. Filter过滤

3. 执行具体的实现类

4. 返回结果

### 异步调用

1. 开启异步调用配置。SSM项目则添加消费<dubbo:method>，设置async=”true”；SpringBoot项目则添加注解@Service(interfaceClass=UserAPI.class,async=true)

2. 调用用户服务接口的UserAPI接口时会异步调用，直接调用不会立马返回结果，需要主动获取。

    ```
    UserDO userDO=UserAPI.register(UserVO);		//此时userDO为空
    Future<UserDO> future=RpcContext.getContext().getFuture();
    UserDO userDO=future.get();		//此时才真正获取到userDO对象。
    ```

### 集群

#### 集群容错方案

**Failover Cluster**：默认，失败自动切换，出现失败时重试其它服务器。通常用于读操作，但重试会带来更长延迟。

**Failfast Cluster**：快速失败，只发起一次调用，失败立即报错。通常用于非幂等性的写操作，比如新增记录。

**Failsafe Cluster**：失败安全，出现异常时，直接忽略。通常用于写入审计日志等操作。

**Failback Cluster**：失败自动恢复，后台记录失败请求，定时重发。通常用于消息通知操作。

**Forking Cluster**：并行调用多个服务器，只要一个成功即返回。通常用于实时性要求较高的读操作，但需要浪费更多服务资源。可通过 forks=”2″ 来设置最大并行数。

**Broadcast Cluster**：广播调用所有提供者，逐个调用，任意一台报错则报错 。通常用于通知所有提供者更新缓存或日志等本地资源信息。

#### 负载均衡策略

​		在调用接口时，会在注册中心获取服务列表，然后缓存在本地，在本地实现远程调用，属于客户端测负载均衡。

**Random LoadBalance**：随机，按权重设计随机概率。

例如：假设我们有一组服务器 servers = [A, B, C]，他们对应的权重为 weights = [5, 3, 2]，权重总和为10。现在把这些权重值平铺在一维坐标值上，[0, 5) 区间属于服务器 A，[5, 8) 区间属于服务器 B，[8, 10) 区间属于服务器 C。接下来通过随机数生成器生成一个范围在 [0, 10) 之间的随机数，然后计算这个随机数会落到哪个区间上。权重越大的机器，在坐标轴上对应的区间范围就越大，因此随机数生成器生成的数字就会有更大的概率落到此区间内。

**RoundRobin LoadBalance**：轮询，按权重设置轮询比例。

**LeastActive LoadBalance**：最少活跃调用数，活跃调用数越小，表明该服务提供者效率越高，单位时间内可处理更多的请求。此时应优先将请求分配给该服务提供者。

**ConstantHash LoadBalance**：一致性哈希，使用相同参数的请求总是会发到同一provider，一台宕机，可基于虚拟节点分摊至其他提供者，避免引起剧烈变动。

#### 加权降权配置

1. 服务提供方暴露服务时，在注解中配置@Service(weight=50)，这样权重写死不能动态调整。
2. 在Dubbo Admin管理控制台可以看到多个服务提供者，为其配置权重。

#### 优先级配置

​		可以在多个配置项设置超时时间等，配置的查找优先级为：方法级优先，接口级次之，全局配置再次之；如果级别一样，则消费方优先，提供方次之。

#### 失效踢出

​		服务提供者的服务失效踢出基于 Zookeeper 的临时节点原理实现。

### 服务降级

​		Dubbo中的服务降级实际上使用的就是Dubbo中的Mock机制，比如当调用某个服务失败后，可以设置`mock=fail:return+null`表示调用服务失败后返回null。

### 源码

#### 服务的注册与发现

​		https://www.jianshu.com/p/1ff25f65587c

​		https://blog.csdn.net/cdw2328/article/details/94769531

### 手写Dubbo

#### 模块

##### Dubbo模块

1. dubbo-interface：定义了一些接口（服务），是单独的模块，会打成jar包。dubbo-provider和dubbo-consumer引用后可直接使用接口。

2. Provider：

    1. 提供接口的实现类

    2. Application启动类，用Spring启动初始化配置文件。

        ```
        new ClassPathXmlApplicationContext(provider配置文件.xml)
        调用start()
        System.in.read();//让线程阻塞，否则方法运行完了什么也没干线程就结束了，这样不至于立马结束掉，便可等待接收请求。
        ```

    3. xml配置文件里把Impl做成了Spring里的Bean对象，配置文件包含暴露服务和注册服务，总称服务导出exported。将服务接口注册到注册中心，同时需要把实现类注册到本地。本地注册就是根据传来的接口名，找到对应实现类，再根据参数类型列表定位到唯一的方法。

        ```
        <dubbo:service interface="包名.接口名" ref="Impl的id">
        //暴露服务：包名.接口名
        //本地注册服务：Impl的id
        ```

3. Consumer：拿接口名从注册中心获取服务地址、调用服务。

    1. xml配置文件中，将引用到的服务注册成Spring中的Bean。

        ```
        <dubbo:reference id="bean名" check="false" interface="包名.接口名">
        ```

    2. Application启动类，直接从容器中取出配置文件中注册号的bean。在实际项目中直接使用的@Autowired起到了同样的作用。

        ```
        new ClassPathXmlApplicationContext(consumer配置文件.xml)
        调用start()
        context.getBean("bean名",接口名.class)
        ```

4. Monitor：监控中心实际上也是一个服务，作为服务提供者，向外提供一个接口供调用。这个角度来看provider和consumer便是monitor的消费者。

5. Registry：保存服务配置信息（服务名：List<'URL>）。

##### 我们的模块

1. Provider

2. Consumer

3. Registry

4. RpcProtocol：要让被人能够调用到你暴露的服务，需要自身能够接收网络请求，所以需要启动一个Tomcat或者Netty接收请求。Dubbo支持可扩展的协议，我们添加一个RpcProtocol模块，这里实现基于Tomcat的HttpProtocol、基于Netty的DubboProtocol。

5. 将一些公共的提到Framework模块。

    //Monitor就不实现了。

#### 初思路

##### http协议+Tomcat实现

1. provider需：

    1. 提供API接口Interface
    2. 提供接口的实现类Impl
    3. 启动类：本地注册+远程注册+启动Tomcat

    ```
    new HttpServer.start("localhost",8080);
    ```

2. 去Protocol模块中建个HttpServer类，写个start()。

    ```
    start(String hostname, Integer port);
    ```

3. 使用内嵌的tomcat，需在外层pom.xml导包<'tomcat-embed-core/>。也可以用外部Tomcat自己控制。

4. start()方法中new一个Tomcat，配置参数，层层加入形成父子节点。相当于外部Tomcat配置文件中的层级节点。

    ```
    Server server = tomcat.getServer();
    Service service = server.findService("Tomcat");
    
    Connector connector = new Connector();
    connector.setPort(port);
    
    Engine engine = new StandardEngine();
    engine.setDefaultHost(hostname);
    
    Host host = new StandardHost();
    host.setName(hostname);
    
    String contextPath = "";
    Context context = new StandardContext();
    context.setPath(contextPath);
    context.addLifecycleListener(new Tomcat.FixContextListener());
    
    host.addChild(context);
    engine.addChild(host);
    
    service.setContainer(engine);
    service.addConnector(connector);
    ```

    ![image-20200430022836009](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20200430022836009.png)

5. Tomcat是个servlet容器，不是web容器，有servlet才能处理请求。所以需要写个Servlet类，重写service()。

    ```
    calss DispatherServlet extends HttpServlet{
    	@Override
    	service(HttpRequest req,HttpResponse res){
    		super Service(req,res);
    	}
    }
    ```

6. 再建个HttpServerHandler类，是核心处理Servlet的类，handler()处理请求。

7. 因为有了HttpServerHandler，DispatherServlet里的service方法实现改成：

    ```
    new HttpServerHandler().handler(req,res);
    ```

8. 在Tomcat里加入写好的servlet实现类。配映射关系，令所有请求都需经过该servlet。

    ```
    tomcat.addServlet(contextPath, "dispatcher",new DispatcherServlet());
    context.addServletMappingDecoded("/*", "dispatcher");
    ```

9. provider需本地注册，我们在provider模块里建个LocalRegister类，维护一个Map<String,Class>成员变量，存服务名和实现类。提供regist()和get()。

10. provider需远程注册，我们在registryr模块里建个RemoteRegister类，维护一个Map<String, List<URL>>成员变量，因为可能是集群，所以用List<'URL>，存服务名和对应的Url列表。提供regist()和get()。

11. 因为服务调用是用API获取URL，所以在framework模块中写个Url类，属性有hostname和port。

12. provider根据一些API获取本机的地址、端口，封装成Url，注册到远程注册中心RemoteRegister上。

13. 我们在Protocol模块中写个HttpClient类，send()需要发送hostname+port+传递的参数。

14. 将传递的参数封装成bean，在framework模块中写个Invocation类。因为需要在网络中传输，需实现序列化接口。属性有：

    ```
    private String interfaceName;
    private String methodName;
    private Object[] params;
    private Class[] paramType;
    ```

15. 在HttpClient的send()里new一个java.net包下的URL，openConnection()，用HttpURLConnection接收指定打开该种连接。将传递的数据转换成输出流。

    ```
    URL url = new URL("http", hostname, port, "/");
    HttpURLConnection httpURLConnection = (HttpURLConnection) url.openConnection();
    
    httpURLConnection.setRequestMethod("POST");
    httpURLConnection.setDoOutput(true);
    
    OutputStream outputStream = httpURLConnection.getOutputStream();
    ObjectOutputStream oos = new ObjectOutputStream(outputStream);
    
    oos.writeObject(invocation);
    oos.flush();
    oos.close();
    ```

16. HttpServerHandler的handler()处理请求，从流取出参数，从本地注册表根据接口名取出实现类，取出方法，执行服务，返回结果给HttpClient。

     ```
     InputStream inputStream = req.getInputStream();
     ObjectInputStream ois=new ObjectInputStream(inputStream);
     
     Invocation invocation=(Invocation)ois.readObject();
     
     Class implClass=LocalRegister.get(invocation.getInterfaceName());
     
     Method method=implClass.getMethod(invocation.getMethodName(),invocation.getParamTypes());
     
     String result=(String)method.invoke(implClass.newInstance(),invocation.getParams());
     
     IOUtils.write(result,resp.getOutputStream());
     ```

17. 在HttpClient的send()里加上用流接收HttpServerHandler返回的结果，并返回最后的结果。

     ```
     InputStream inputStream = httpURLConnection.getInputStream();
     String result = IOUtils.toString(inputStream);
     return result;
     ```

18. Consumer用写好的HttpClient工具调用服务

     ```
     HttpClient httpClient=new HttpClient();
     Invocation invocation=new Invocation(接口名.class.getName(),方法名,参数类型数组，参数数组);
     String result=httpClient.send("主机名",端口号);
     ```

19. Consumer里的调用其实就是使用网络调用远程的服务，可以理解为从Spring里取出一个代理对象，再用代理对象完成一些网络的处理。所以我们可以在framework模块写个代理工厂类ProxyFactory，这样使得consumer的调用更加透明入侵小。这里使用JDK动态代理。

    ```
    public static <T> T getProxy(final Class interfaceClass) {
    	return (T) Proxy.newProxyInstance(interfaceClass.getClassLoader(), new Class[]{interfaceClass}, new InvocationHandler() {
    		@Override
    		public Object invoke(Object proxy, Method method, Object[] args){
    			HttpClient httpClient=new HttpClient();
    			Invocation invocation=new Invocation(接口名.class.getName(),方法名,参数类型数组，参数数组);
    			String result=httpClient.send("主机名",端口号);
    			return result;
    		}
    	});
    }
    ```

20. Consumer里直接这样调用

    ```
    HelloService helloService=ProxyFactory.getProxy(HelloService.class);
    String result = helloService.sayHello("周瑜");
    ```

21. 写死不好，优化一下，在RemoteMapRegister里加个负载均衡算法，Consumer改成从注册中心查，并负载均衡。

    ```
    //RemoteMapRegister里：
    public static Url random(String,interfaceName){
    	List<Url> list=REGISTER.get(interfaceName);
    	Random random=new Random();
    	int n=random.nextInt(list.size());
    	return list.get(n);
    }
    ```

    ```
    //Consumer里：
    Url url=RemoteMapRegister.random(interfaveClass.getName());
    String result = httpClient.send(url.getHostname(),url.getPort(),invocation);
    ```

22. 由于provider和consumer是两个进程，provider启动时把信息存入注册中心的map里，这个map是不共享的，在consumer取出来会是空的。Dubbo用zookeeper或redis做注册中心，我们这里几个进程在一台机器，所以我们用文件共享解决，map存完持久化到文件中。所以Url类需实现序列化接口。

    ```
    private static void saveFile() {
    	FileOutputStream fileOutputStream = new FileOutputStream("/temp.txt");
    	ObjectOutputStream objectOutputStream = new ObjectOutputStream(fileOutputStream);
    	objectOutputStream.writeObject(REGISTER);
    }
    
    private static Map<String, List<URL>> getFile() {
    	FileInputStream fileInputStream = new FileInputStream("/temp.txt");
    	ObjectInputStream objectInputStream = new ObjectInputStream(fileInputStream);
    	return (Map<String, List<URL>>) objectInputStream.readObject();
    }
    ```

##### dubbo协议+Netty实现

1. 写一个NettyServer

2. 写一个NettyServerHandler，extends ChannelInboundHandlerAdapter。

    ```
    @Override
    public void channelRead(ChannelHandlerContext ctx, Object msg){
    	Invocation invocation = (Invocation) msg;
    
    	Class serviceImpl=LocalRegister.get(invocation.getInterfaceName());
    
    	Method method = serviceImpl.getMethod(invocation.getMethodName(), invocation.getParamType());
    	Object result = method.invoke(serviceImpl.newInstance(), invocation.getParams());
    
    	System.out.println("Netty-------------" + result.toString());
    	ctx.writeAndFlush("Netty:" + result);
    ```

#### 可拓展思路

1. 切换协议时，需要改动ProxyFactory的proxy()方法，和Provider的main()方法，将里面的new HttpClient()和new HttpServer()改成new NettyClient()和new NettyServer()。我们简单模拟一下Dubbo的SPI可扩展机制思想。首先把共同的东西抽象出来，建一个协议接口Protocol，含start()和send()方法。

    ```
    void start(URL url);
    String send(URL url, Invocation invocation);
    ```

2. 建HttpProtocol和DubboProtocol分别实现Protocol。

    ```
    @Override
    public void start(URL url) {
    	HttpServer httpServer = new HttpServer();
    	httpServer.start(url.getHostname(), url.getPort());
    }
    
    @Override
    public String send(URL url, Invocation invocation) {
    	HttpClient httpClient = new HttpClient();
    	return httpClient.send(url.getHostname(), url.getPort(),invocation);
    }
    ```

3. 这样Provider的start()，和ProxyFactory的proxy()便可更透明一些：

    ```
    Protocol protocol = new HttpProtocol();
    protocol.start(url);
    protocol.send(url，invocation);
    ```

4. 但是上面这样写切换协议还是需要手动改代码，我们用个工厂类改进一下，建个ProtocolFactory类，getProtocol()方法，直接从系统的属性取所指定的协议，可以在启动项目时加上-D参数指定协议，或者在IDE的Configurations面板的VM options栏指定-DprotocolName="协议名"。

    ```
    public static Protocol getProtocol() {
        // 工厂模式
        String name = System.getProperty("protocolName");
        if (name == null || name.equals("")) name = "http";
        switch (name) {
            case "http":
                return new HttpProtocol();
            case "dubbo":
                return new DubboProtocol();
            default:
                break;
        }
        return new HttpProtocol();
    }
    ```

5. 这样Provider的start()，和ProxyFactory的proxy()便可更透明一些：

    ```
    Protocol protocol = ProtocolFactory.getProtocol();
    protocol.start(url);
    protocol.send(url，invocation);
    ```

6. 上述这样可以解决切换协议的问题，但局限是新增协议还需改switch的条件。我们使用Java的SPI机制，SPI机制可实现可扩展机制。

7. 在ProtocolFactory的getProtocol()方法中，使用ServiceLoader类的load()会加载传入的类名的实现类，返回其Impl实例。具体会从resources/META-INF/services包下找到文件名为传入的接口的全类名的文件，该文件内写了其实现类的全类名。

    ```
    //Java.SPI机制
    ServiceLoader<Protocol> serviceLoader=ServiceLoader.load(Protocol.class);
    Iterator<Protocol> iterator=serviceLoader.iterator();
    return iterator.next();
    ```

#### Java SPI

​		数据库JDBC的可扩展便是用的SPI机制，面向接口编程，不关心使用的数据库到底是mysql还是sqlite，只需引入对应数据库的jar包，包里有个文件会指定Driver接口的实现类。

​		但是Java SPI的局限是不能单独的获取某个指定的实现类，以及没有IOC和AOP机制。当有多个实现类时，会加载进来多个实现类，但是文件中并没有定义别名之类的属性，我们实际使用只需要其中一个实现类，但不好取出指定的实现类。当需要在Impl周围执行一些其他操作，Java SPI也没有提供AOP机制。

##### ServiceLoader源码

1. load()方法调用reload()
2. reload()里，new LazyIterator(service,loader)。
3. LazyIterator先调用next()
4. next()调用hasNextService()
5. hasNextService()里：
    1. 前缀（即包名）+service.getName()，拼接得到fullName，即文件名。
    2. 如果loader是null，则传入文件名，调用ClassLoader.getSystemResources(fullName)，加载文件，得到configs。
    3. 遍历解析文件元素。

6. 下一个执行nextService()：
    1. 传入loader，调用Class.forName(loader)，得到Class对象。 
    2. 调用newInstance()，得到实例并返回。

#### 代码

##### framework

```
@Data
@AllArgsConstructor
public class URL implements Serializable {
    private String hostname;
    private Integer port;
}
```

```
@Data
@AllArgsConstructor
public class Invocation implements Serializable {
    private String interfaceName;
    private String methodName;
    private Object[] params;
    private Class[] paramType;
}
```

```
public class LoadBalance {
    public static URL random(List<URL> list) {
        Random random =new Random();
        int n = random.nextInt(list.size());
        return list.get(n);
    }
}
```

```
public interface Protocol {
    void start(URL url);
    String send(URL url, Invocation invocation);
}
```

```
public class ProtocolFactory {
	public static Protocol getProtocol() {
    	// 工厂模式
    	String name = System.getProperty("protocolName");
    	if (name == null || name.equals("")) name = "http";
    	switch (name) {
        case "http":
            return new HttpProtocol();
        case "dubbo":
            return new DubboProtocol();
        default:
            break;
    	}
    	return new HttpProtocol();
	}
}
```

```
public class ProxyFactory {
    public static <T> T getProxy(final Class interfaceClass) {
        return (T) Proxy.newProxyInstance(interfaceClass.getClassLoader(), new Class[]{interfaceClass}, new InvocationHandler() {
            @Override
            public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {
                String mock = System.getProperty("mock");
                if (mock.startsWith("return:")) {
                    String result = mock.replace("return:", "");
                    return result;
                }

                Invocation invocation = new Invocation(interfaceClass.getName(), method.getName(), args, method.getParameterTypes());
                List<URL> urlList = ZookeeperRegister.get(interfaceClass.getName());
                URL url = LoadBalance.random(urlList);
                System.out.println(url);
                Protocol protocol = ProtocolFactory.getProtocol();
                String result = protocol.send(url, invocation);
                return result;
            }
        });
    }
}

```

##### protocol

###### dubbo

```
public class DubboProtocol implements Protocol {
    @Override
    public void start(URL url) {
        NettyServer nettyServer = new NettyServer();
        nettyServer.start(url.getHostname(), url.getPort());

    }

    @Override
    public String send(URL url, Invocation invocation) {
        NettyClient nettyClient = new NettyClient();
        return nettyClient.send(url.getHostname(),url.getPort(), invocation);
    }
}
```

```
public class NettyClient<T> {
    public NettyClientHandler client = null;
    private static ExecutorService executorService = Executors.newCachedThreadPool();

    public void start(String hostName, Integer port) {
        client = new NettyClientHandler();
        Bootstrap b = new Bootstrap();
        EventLoopGroup group = new NioEventLoopGroup();
        b.group(group)
                .channel(NioSocketChannel.class)
                .option(ChannelOption.TCP_NODELAY, true)
                .handler(new ChannelInitializer<SocketChannel>() {
                    protected void initChannel(SocketChannel socketChannel) {
                        ChannelPipeline pipeline = socketChannel.pipeline();
                        pipeline.addLast("decoder", new ObjectDecoder(ClassResolvers
                                .weakCachingConcurrentResolver(this.getClass()
                                        .getClassLoader())));
                        pipeline.addLast("encoder", new ObjectEncoder());
                        pipeline.addLast("handler", client);
                    }
                });

		b.connect(hostName, port).sync();
    }

    public String send(String hostName, Integer port, Invocation invocation) {
        if (client == null) {
            start(hostName, port);
        }
        client.setInvocation(invocation);
		return (String) executorService.submit(client).get();
    }
}
```

```
public class NettyClientHandler extends ChannelInboundHandlerAdapter implements Callable {
    private ChannelHandlerContext context;
    private Invocation invocation;
    private String result;

    @Override
    public void channelActive(ChannelHandlerContext ctx) throws Exception {
        context = ctx;
    }

    @Override
    public synchronized void channelRead(ChannelHandlerContext ctx, Object msg){
        result = msg.toString();
        notify();
    }

    public synchronized Object call() throws Exception {
        context.writeAndFlush(this.invocation);
        wait();
        return result;
    }

    public void setInvocation(Invocation invocation) {
        this.invocation = invocation;
    }
}
```

```
public class NettyServer {
    public void start(String hostName, int port) {
		final ServerBootstrap bootstrap = new ServerBootstrap();
        NioEventLoopGroup eventLoopGroup = new NioEventLoopGroup();
        bootstrap.group(eventLoopGroup)
               .channel(NioServerSocketChannel.class)
               .childHandler(new ChannelInitializer<SocketChannel>() {
                    protected void initChannel(SocketChannel socketChannel) {
                        ChannelPipeline pipeline = socketChannel.pipeline();
                        pipeline.addLast("decoder", new ObjectDecoder(ClassResolvers
                              .weakCachingConcurrentResolver(this.getClass()
                                       .getClassLoader())));
                        pipeline.addLast("encoder", new ObjectEncoder());
                        pipeline.addLast("handler", new NettyServerHandler());
                   }
               });
        bootstrap.bind(hostName, port).sync();
    }

    public void close() {
    }
}
```

```
public class NettyServerHandler extends ChannelInboundHandlerAdapter {
    @Override
    public void channelRead(ChannelHandlerContext ctx, Object msg) {
        Invocation invocation = (Invocation) msg;

        Class serviceImpl = LocalRegister.get(invocation.getInterfaceName());

        Method method = serviceImpl.getMethod(invocation.getMethodName(), invocation.getParamType());
        Object result = method.invoke(serviceImpl.newInstance(), invocation.getParams());

        ctx.writeAndFlush("Netty:" + result);
    }
}
```

###### http

```
public class DispatcherServlet extends HttpServlet {
    @Override
    protected void service(HttpServletRequest req, HttpServletResponse resp) {
        new HttpServerHandler().handler(req, resp);
    }
}
```

```
public class HttpClient {
	public String send(String hostname, Integer port, Invocation invocation) {
		var request = HttpRequest.newBuilder()
        .uri(new URI("http", null, hostname, port, "/", null, null))
        .POST(HttpRequest.BodyPublishers
        	.ofString(JSONObject.toJSONString(invocation)))
        .build();
		var client = java.net.http.HttpClient.newHttpClient();

		HttpResponse<String> response = 
		client.send(request,HttpResponse.BodyHandlers.ofString());

		String result = response.body();
	}
}
```

```
 class HttpProtocol implements Protocol {
    @Override
    public void start(URL url) {
        HttpServer httpServer = new HttpServer();
        httpServer.start(url.getHostname(), url.getPort());
    }

    @Override
    public String send(URL url, Invocation invocation) {
        HttpClient httpClient = new HttpClient();
        return httpClient.send(url.getHostname(), url.getPort(),invocation);
    }
}
```

```
public class HttpServer {
    public void start(String hostname, Integer port) {

        Tomcat tomcat = new Tomcat();

        Server server = tomcat.getServer();
        Service service = server.findService("Tomcat");

        Connector connector = new Connector();
        connector.setPort(port);

        Engine engine = new StandardEngine();
        engine.setDefaultHost(hostname);

        Host host = new StandardHost();
        host.setName(hostname);

        String contextPath = "";
        Context context = new StandardContext();
        context.setPath(contextPath);
        context.addLifecycleListener(new Tomcat.FixContextListener());

        host.addChild(context);
        engine.addChild(host);

        service.setContainer(engine);
        service.addConnector(connector);

        tomcat.addServlet(contextPath, "dispatcher", new DispatcherServlet());
        context.addServletMappingDecoded("/*", "dispatcher");

		tomcat.start();
		tomcat.getServer().await();
    }
}
```

```
public class HttpServerHandler {

    public void handler(HttpServletRequest req, HttpServletResponse resp) {
		Invocation invocation = JSONObject
		.parseObject(req.getInputStream(), Invocation.class);
		var interfaceName = invocation.getInterfaceName();
		var implClass = LocalRegister.get(interfaceName);
		var method = implClass
		.getMethod(invocation.getMethodName(), invocation.getParamType());
		var result = (String) method
		.invoke(implClass.newInstance(), invocation.getParams());

		IOUtils.write(result, resp.getOutputStream());
    }
}
```

##### register

```
public class RemoteMapRegister {
    private static Map<String, List<URL>> REGISTER = new HashMap<>();

    public static void regist(String interfaceName, URL url){
        List<URL> list = REGISTER.get(interfaceName);
        if (list == null) {
            list = new ArrayList<>();
        }
        list.add(url);
        REGISTER.put(interfaceName, list);
        saveFile();
    }

    public static List<URL> get(String interfaceName) {
        REGISTER = getFile();
        List<URL> list = REGISTER.get(interfaceName);
        return list;
    }


    private static void saveFile() {
		FileOutputStream fileOutputStream = new FileOutputStream("/temp.txt");
		ObjectOutputStream objectOutputStream = 
		new ObjectOutputStream(fileOutputStream);
		objectOutputStream.writeObject(REGISTER);
    }

    private static Map<String, List<URL>> getFile() {
		FileInputStream fileInputStream = new FileInputStream("/temp.txt");
		ObjectInputStream objectInputStream = new ObjectInputStream(fileInputStream);
		return (Map<String, List<URL>>) objectInputStream.readObject();
    }
}
```

```
public class ZookeeperRegister {
    static CuratorFramework client;

    static {
        client = CuratorFrameworkFactory
                .newClient("localhost:2181", new RetryNTimes(3, 1000));
        client.start();

    }

    private static Map<String, List<URL>> REGISTER = new HashMap<>();

    public static void regist(String interfaceName, URL url) {
		String result = client.create().creatingParentsIfNeeded()
		.withMode(CreateMode.EPHEMERAL)
		.forPath(String.format("/dubbo/service/%s/%s", interfaceName, 
			JSONObject.toJSONString(url)), null);
    }

    public static List<URL> get(String interfaceName) {
        List<URL> urlList = new ArrayList<>();
		List<String> result = client.getChildren().forPath(
			String.format("/dubbo/service/%s", interfaceName));
		for (String urlstr : result) {
			urlList.add(JSONObject.parseObject(urlstr, URL.class));
		}
		REGISTER.put(interfaceName, urlList);
        return urlList;
    }
}
```

##### consumer

```
public class Consumer {
    public static void main(String[] args) {
		String result = helloService.sayHello("周瑜");
    }
}
```

##### provider

```
public class LocalRegister {
    private static Map<String, Class> map = new HashMap<>();

    public static void regist(String interfaceName, Class implClass) {
        map.put(interfaceName, implClass);
    }

    public static Class get(String interfaceName) {
       return map.get(interfaceName);
    }
}
```

```
public class Provider {
    private static boolean isRun = true;

    public static void main(String[] args) {
        // 注册服务
        URL url = new URL("localhost", Integer.valueOf(System.getProperty("port")));
      
		//RemoteMapRegister.regist(HelloService.class.getName(), url);
        ZookeeperRegister.regist(HelloService.class.getName(), url);
        LocalRegister.regist(HelloService.class.getName(), HelloServiceImpl.class);

        // 启动Tomcat
        Protocol protocol = ProtocolFactory.getProtocol();
        protocol.start(url);
    }
}
```

###### api

```
public interface HelloService {
    String sayHello(String userName);
}
```

###### impl

```
public class HelloServiceImpl implements HelloService {
    @Override
    public String sayHello(String userName) {
        return "Hello: " + userName;
    }
}
```

### Dubbo源码

#### Dubbo SPI

​		Dubbo的服务注册、暴露、发现均是基于Dubbo SPI机制实现的。

##### 可扩展机制

1. 为了解决Java SPI机制的局限，Dubbo自己实现了Dubbo SPI机制，解决了这种问题，并且额外提供了以下功能：

    1. 当实现类中依赖了其他的接口，Dubbo SPI提供了自动注入。
    2. Dubbo SPI提供了AOP功能。
    3. Dubbo SPI性能更高。

2. Dubbo SPI的resources/META-INF/services/下的文件里，是以key=value的形式。

3. 调用时便可根据key得到Impl对象，再用该对象调用方法。

    ```
    ExtensionLoader.getExtensionLoader(接口名.class).getExtension("key");
    ```

4. 需在接口上添加注解@SPI，标识该接口是可扩展的。

##### AOP

1. 当Impl周围需执行一些其他操作时。接口名Wrapper包装类，去实现接口，包装该接口所有的实例。里面在调用方法前后加一些操作。需把包装类加到配置文件中。

##### IOC

1. 若实现类里依赖了其他的接口，到底依赖哪个实现类呢？
    1. Dubbo里有个URL总线类，指定服务名、地址、参数map（包含协议、实现类）。
    2. map里用type作为key，将map放到URL中，指定到底是哪个Impl，调用方法时传入URL。
    3. type标在接口的方法上的注解里，@Adaptive(value="type")。
    4. 可指定该接口的默认实现类，需在接口上添加注解，@SPI(value="指定默认Impl")。

2. 具体实现：

    1. 每个接口对应一个ExtensionLoader扩展加载器。调用getExtensionLoader()，传入接口类型。

        ```
        ExtensionLoader.getExtensionLoader(接口名.class);
        ```

    2. getExtensionLoader()里有缓存机制，把每个类的加载器都缓存起来，获取加载器时先先试着取，没有就先put进map里，再new一个加载器ExtensionLoader并返回。

    3. ExtensionLoader类有个ObjextFactory属性，构造方法里会先判断如果传进来的是ExtensionFactory，则返回空，否则先取出接口的加载器，调用其方法，取出代理类并返回。

        ```
        ExtensionLoader.getExtensionLoader(ExtensionFactory.class).getAdaptiveExtensio();
        ```

    4. 传入type，调用getExtension()，获取指定的Impl

        ```
        extensionLoader.getExtension("type");
        ```

    5. getExtension()先判断传入的type是否为true。若名字为true，则获取默认的Impl，即@SPI中value属性指定的。否则，用双重空检查保证单例模式。

        ```
        Holder holder=getOrCreateHolder(type);
        Objext instance=holder.get();
        if(instance==null){
        	synchronized(holder){
        		instance=holder.get();
        		if(instance==null){
        			instance=createExtension(type);
        			holder.set(instance);
        		}
        	}
        }
        ```

    6. createExtension()方法里根据名字返回一个类

        ```
        Class clazz=getExtensionClasses.get(type);
        ```

    7. getExtensionClasses()会返回一个map，这个map实际就是文件里面所有的参数的map形式，该方法里有利用了缓存。Dubbo里面有很多缓存，因为解析很慢。
    8. loadExtensionClasses()方法里，会去路径列表逐个加载目录，拿到文件的地址。
    9. loResource()方法，把文件的元素逐个取出放到map，所以我们在文件中写同样的key可能会覆盖，因此有优先级。
    10. 把每一行转成类对象，loadClass：
        1. 验证Impl是否为Interface的派生类
        2. 验证是否有Adaptive注解，若有说明需代理。对于一个接口，只能有一个proxy。
        3. 判断Impl是否是包装类，方法时判断构造函数是否和类名相同，不同则是包装类。
        4. 判断有无构造方法。

    11. 若没有手动写proxy，dubbo会自定实现一个，generate()。
    12. 若Impl里依赖了其他类，dubbo SPI可自动注入。

### 项目中遇到的问题

Q. dubbo社区不活跃了，有一段时间没更新了，项目使用dubbo有没遇到什么问题？

A. 通常开发时会把POJO、自定义异常这些共用的类放在common包里，给各个模块依赖。当我们在provider的实现类方法里throw出一个自定义的RuntimeException时，consumer调用provider却catch不到该异常。解决办法：

1. 异常栈信息发现：`com.alibaba.dubbo.rpc.filter.ExceptionFilter.invoke(ExceptionFilter.java:108)`
2. 找到源码的invoke()方法：
    1. 如果是checked异常，直接抛出。
    2. 在方法签名上有声明，直接抛出。
    3. 异常类和接口类在同一jar包里，直接抛出。
    4. 是JDK自带的异常，直接抛出。
    5. 是Dubbo本身的异常(RpcException)，直接抛出。
    6. 否则，包装成RuntimeException抛给客户端。

3. 显然我们的自定义RuntimeException，不是编译时异常、未在接口中声明该异常、异常类在common.jar而不在接口所在的jar、不是JDK自带的异常、不是Dubbo自带的RpcException异常。所以最后被包装成了RuntimeException异常抛出。这也就是为什么我们catch`HelloException`是catch不到的，因为被包装成了`RuntimeException`。
4. 其实Dubbo的这个设计是基于序列化来考虑的。如果provider抛出一个仅在provider自定义的一个异常，那么该异常到达consumer，明显是无法序列化的。Dubbo的判断：
    1. checked异常和RuntimeException是不同类型，强行包装可能会出现类型转换错误，因此不包，直接抛出。
    2. 若方法签名上有声明时，如果这个异常是provider.jar中定义的，因为consumer是依赖api.jar的，而不是依赖provider.jar，那么编译都编译不过，如果能编译得过，说明consumer是能依赖到这个异常的，因此序列化不会有问题，直接抛出。
    3. 若异常类和接口类在同一jar包里，provider和consumer都依赖api，如果异常在这个api，那序列化也不会有问题，直接抛出。
    4. 若是JDK自带的异常，直接抛出。provider和consumer都依赖jdk，序列化也不会有问题，直接抛出。
    5. 若是Dubbo本身的异常(RpcException)，provider和consumer都依赖Dubbo，序列化也不会有问题，直接抛出。
    6. 否则，包装成RuntimeException抛给客户端。此时就有可能出现我们遇到的情况，这个异常是provider.jar自定义的，那么provider抛出的时候进行序列化，因为consumer没有依赖provider.jar，所以异常到达consumer时，根本无法反序列化。但是包装成了`RuntimeException`异常则不同，此时异常就是JDK中的类了，到哪都能序列化。保住了dubbo网络传输中的安全可靠。
5. 解决办法：比如从规范上要求业务方接口需声明该自定义异常。

A. 实际中并未遇到其他的问题了，dubbo是个专做远程服务调用的RPC框架，事先了解到其比较依赖第三方组件的可靠性，当第三方组件出现问题时服务易中断，所以我们配置zookeeper和redis时比较谨慎，尽量的去保证其可用性。

Q. 项目dubbo用的什么协议和序列化方式

A. 用的dubbo协议，单一长连接和 NIO 异步通讯，适合大并发小数据量的服务调用，及消费者远大于提供者。传输协议 TCP，异步 Hessian 序列化，性能好。

## Nginx

### 代理

#### 正向代理

​		客户端不能直接访问目标服务器，为了从目标服务器获取内容，客户端向代理服务器发送请求，需指定目标服务器并设置代理服务器的ip或域名，代理服务器将请求转发给目标服务器并将获得的内容返回给客户端。如VPN。

#### 反向代理

​		客户端向反向代理服务器发送请求，反向代理服务器判断请求的走向，转发给相应的服务器，并将请求结果返回给客户端，客户端无需设置，直接访问服务器真实ip或域名即可，代理服务器对外表现为一个服务器，用户不会感知到反向代理后面最终访问的真实机器 。

### 负载均衡

​		通过反向代理将请求按一定策略转发到不同的服务器上，实现服务的负载均衡，减轻每台服务器的压力，避免服务器单节点故障，通过多台服务器共同工作，提高系统吞吐量。

#### 策略

**轮询**：默认策略，将请求按时间顺序轮流分配到不同的后端服务器上，若后端服务器down掉，则自动剔除。轮询法分配均衡，不关心服务器实际的连接数和当前系统负载。

**加权轮询**：不同的后端服务器可能机器的配置和系统的负载不同，因此其抗压能力也不同。加权轮询将请求按顺序按权重分配给后端服务器，常给配置高、负载低的机器分配较高的权重，让其处理更多的请求，反之同理。

**ip_hash源地址哈希法**：根据客户端的ip地址，通过哈希函数计算得到一个数值，对服务器数量取模，便得到要访问的后端服务器的序号。同以ip地址的客户端，当后端服务器列表不变时，请求每次都会映射到同一台后端服务器，可解决session问题。

**随机**：通过系统的随机算法，根据后端服务器的数量随机选取一台进行访问。

**least_conn最小连接数法**：由于后端服务器的配置不尽相同，对于请求的处理速度不同，最小连接数法根据服务器当前的连接情况，动态的选取其中当前积压连接数最少的一台服务器处理当前请求，尽可能提高后端服务的利用效率，将负载合理分流到每台服务器。

#### 配置

​		通过配置文件upstream参数添加应用服务器的ip并在其后添加指定参数。

***e.g.*** upstream serviceCluster{ server ip:port weight=n;}

### 动静分离

​		根据一定规则将动态网站里的动态页面拆分出不变的资源和常变动的资源，图片、html等静态资源交由nginx处理，jsp、servlet等动态请求交由tomcat处理。

#### 配置

1. location / 全部请求交由tomcat处理
2. location ~ .*\.(htm|html|jpg|jpeg|png|ioc|rar)${root /usr/local/webapps; expires 30d;}。静态资源交给nginx处理。里面配置nginx访问的目录，即静态资源所在的目录；并指定这些资源文件在客户端浏览器的缓存时间。

### 获取真实ip

​		在判断异地登录、统计ip访问次数等业务场景，需获取客户ip。使用nginx反向代理后，getRemoteAddr()获取的是nginx服务器的ip，若想获取用户的真实ip，需修改配置文件，加一个自定义变量proxy_set_header X-real-ip $remote_addr;，X-real-ip为随意取的自定义变量名，web端通过request.getAttribute(“X-real-ip”)即可取出真实的ip地址。

### 防刷限流

​		修改nginx配置文件，ngx_http_limit_conn_module模块，http块配置如下，请求超出连接范围返回503。

1. **限制瞬时连接数**：limit_req_zone $binary_remote_addr zone=addr: 10m;；server块配置location /{limit_conn addr 连接数}；
2. **限制每个ip每秒平均请求数**：limit_req_zone $binary_remote_addr zone=allips:10m rate=Nr/s，即限制平均每秒请求数为N；server块配置location /{limit_req burst=M nodelay}，burst为漏桶数，设置nodelay则严格按平均速率限制请求数，否则允许每秒上下浮动burst个。

## Zookeeper

### Zookeeper简介

​		zookeeper是一个开源的分布式协调服务，数据保存在内存中，保证高吞吐量和低延迟。底层提供数据管理和数据节点监听服务。分布式应用程序还可以基于zookeeper实现分布式锁、高可用、数据发布/订阅、通知、元数据及配置信息管理等功能。

#### 特点

**顺序一致性**：同一客户端发起的事务请求，严格按顺序执行。

**原子性**：事务请求在整个集群中所有机器的应用情况一致，要么均应用，要么均未应用。

**单一系统映像**：客户端连接到集群中任意一个服务器，看到的数据模型一致。

**可靠性**：一次更改请求被应用，其更改的结果会被持久化，直到下一个更改覆盖。

#### CAP实现

C：一致性consistency

A：可用性available

P：分区容错性partition tolerance

CAP理论为最多同时满足其2，P是必须满足的。

​		zookeeper保证的是CP。任何时刻对zookeeper的访问请求总能得到一致的数据结果，同时系统对网络分割具备容错性，但不能保证每次服务请求的可用性。极端情况下，zookeeper会丢弃一些请求，consumer需重新请求才能获得结果；再者leader选举过程中集群均不可用，当使用zookeeper获取服务器列表时，leader因网络故障与其他节点失去连续，剩余节点重新选举leader的过程中若所耗时间过长，导致选举期间注册服务瘫痪，虽然服务器最终能够恢复。

#### 作用

​		zookeeper是一个开源的分布式协调服务，主要作用是分布式协调，作为注册中心为分布式环境下各个微服务之间的调用提供协调工作。分布式应用程序还可以基于zookeeper实现分布式锁、高可用、数据发布/订阅、通知、元数据及配置信息管理等功能。

##### 分布式协调

​		zookeeper常用于担任服务生产者和服务消费者的注册中心，provider将自己提供的服务注册到zookeeper中心，consumer在进行服务调用时先到zookeeper中查找服务，获取到provider的详细信息之后再去调用provider的内容与数据。

​		例如：系统A发送请求之后在zookeeper上对某个节点的值注册一个监听器，一旦系统B处理完了就修改该节点的值，系统A便可马上收到通知。

##### HA高可用

​		High Availability，像Hadoop等大数据系统，通常会基于zookeeper来开发高可用机制，即一个重要的进程通常会做主备两个，主进程挂了立马通过zookeeper感知来切换到备用进程。

​		例如：系统A（主）创建临时节点active，值为workerA，系统B（备）给active节点注册监听器，其余什么也不干，A挂掉则临时节点被删除，zookeeper发通知给B，B再次创建同名节点active，值为workerB。

##### 元数据及配置信息管理

​		zookeeper 可以用作很多系统的配置信息的管理，比如 kafka等分布式系统都会选用 zookeeper 来做一些元数据、配置信息的管理，包括 dubbo 注册中心。

##### 实现分布式锁

​		zookeeper内部为一个分层的文件系统目录树结构，规定在同一目录下只能有一个唯一的文件名。利用有序节点+watch实现分布式锁：创建一个目录，每个线程生成一个有序的临时节点，为确保有序性，再排序一次全部节点，获取全部节点，每个线程判断自己是否是最小的，若是则获得锁，执行操作，执行完毕删除自身节点；若不是则监听它的前一个节点，当前一个节点被删除时，则获得锁，以此类推。zookeeper实现的分布式锁性能比redis实现的稍差，但不易死锁，可靠性好。

### 会话

​		Session即zookeeper服务器与客户端会话。服务端为每个客户端分配标识性的sessionID。客户端启动后，与服务器建立TCP长连接，会话开始；通过该连接，客户端通过心跳检测与服务器保持有效的会话，也能向zookeeper服务器发送请求并接收响应，还能接收来自服务端的监听器Watch事件通知。sessionTimeout值可设置会话超时时间。

### 数据单元

​		zookeeper中的节点即数据单元ZNode，分为临时节点、临时顺序节点、持久节点、持久顺序节点。数据模型为树ZNode Tree

#### 临时节点

​		临时节点的生命周期与客户端会话绑定，客户端会话失效则临时节点自动清除。临时节点下不能创建子节点。

#### 持久节点

​		节点创建后就一直存在，直到有删除操作主动清除该节点。

### 集群

​		为了保证高可用，最好部署zookeeper集群，由于其容错性，只要集群中大部分机器是可用的，则zookeeper仍可用。ZooKeeper集群有Leader、Follower、Observer三种角色。集群中所有的非Observer机器通过Leader选举选定一台机器作为Leader。Leader可为客户端提供读和写服务，负责投票的发起和决议，更新系统状态。Follower和Observer只提供读服务。Observer不参与Leader的选举，也不参与过半写成功策略，其作用是扩展系统，提升集群的读性能。

#### 选举机制和节点数配置

​		zookeeper有三种关于leader的选举机制：LeaderElection、AuthFastLeaderElection、FastLeaderElection（默认）。zookeeper容错指当集群down掉几个server后，剩下的server数需大于down掉的个数，即剩下的服务数需大于n/2，zookeeper才可继续使用。对于相同的最大容错服务器个数，奇数个server会比偶数个节省资源。此外，无论奇偶数都可以选举leader，为了保证选举最后能选出leader，则不能出现两台机器得票相同的僵局，所以要求集群的server数最好为奇数；若集群出现问题，其中存活的机器至少得有2台，否则leader无法获得半数以上server的支持，系统就自动挂掉。所以一般是3个或以上个节点。

#### 宕机

​		若集群中某台zookeeper挂掉，自动切换到另一台；若全部挂到，也不影响使用，dubbo在启动时，consumer会从zookeeper拉取注册的producer的地址接口等数据，缓存到本地，每次调用时按本地存储的地址进行调用。

### ZAB

#### ZAB协议

​		ZAB（ZooKeeper Atomic Broadcast）原子广播协议是ZooKeeper专门设计的一种支持崩溃恢复的原子广播协议。基于该协议，ZooKeeper实现了一种主备模式的系统架构来保持分布式系统下集群中各个副本之间的数据一致性。zab核心思想是将所有写操作的请求转换为事务proposal。leader写完数据后向所有follower发送数据广播请求，得到半数以上反馈后向follower发送commit消息，leader间上的数据从而同步到follower节点上。

#### 消息广播

​		ZooKeeper设计成只允许唯一一个Leader服务器来进行事务请求的处理。

1. leader服务器将客户端的写操作请求转化为事务proposal提案，并分配全局唯一的事务ID（ZXID），可保证顺序。
2. leader为每个follower分配一个queue，将需要广播的proposal依次放入，根据FIFO策略进行消息发送。
3. follower接收到proposal后将其以事务日志的形式写入本地磁盘，成功写入后反馈给leader一个ack响应。
4. leader接收到半数以上follower的ack后，广播一个commit消息给所有follower，通知它们进行事务提交，同时自身完成对事务的提交。
5. 每个follower接收到commit消息后，完成对事务的提交。

#### 崩溃恢复

​		当整个服务框架在启动过程中，或是当 Leader 服务器出现网络中断、崩溃退出与重启等异常情况时，ZAB 协议就会进入恢复模式并选举产生新的Leader服务器。当选举产生了新的 Leader后，同时集群中已经有过半的机器与该Leader服务器完成了数据同步之后，ZAB协议就会退出恢复模式。

### 集成

#### Dubbo调用ZooKeeper

```
<dubbo:registry protocol="zookeeper" address="${ZOOKEEPER_REGISTRY_URL}" />
//说明使用的注册中心是zookeeper；address是去哪里注册服务的地址。
```

提供服务的配置文件：

```
<dubbo:service interface="UsrBasicService全限定名" ref="usrBasicService"></dubbo:service> 
<bean id="usrBasicService" class="UsrBasicServiceImpl全限定名"/>
```

使用服务的配置文件：

```
<dubbo:reference id="usrBasicService" interface="UsrBasicService全限定名" timeout="3000000" check="false" />
```

## RocketMQ

### RocketMQ简介

#### 技术选型

**RocketMQ**：阿里开发的；单机吞吐量10w级；同等机器下可支撑大量topic保证吞吐量；分布式架构，可用性非常高；经参数优化消息可0丢失；延迟ms级；功能完善，扩展性好，支持延迟消息、事务消息、消息回溯、死信队列、消息积压等高级功能；Java语言编写的，可自行维护。问题是社区活跃度不高，阿里捐给Apache，适合架构研发实力强的大公司用。

**Kafka**：单机吞吐量10w级；同等机器下可支撑的topic数有限；分布式架构，可用性非常高；数据先写入磁盘缓冲区，未直接落盘，可能会丢失数据，适合大数据等适当丢失数据没关系、吞吐量要求高、不需要太多高级功能的场景；延迟ms级；功能较简单，主要用在大数据领域。

**RabbitMQ**：单机吞吐量w级；主从架构；延迟微秒级，性能最好，适合对并发和吞吐量要求不是很高的小公司。

**ActiveMQ**：单机吞吐量w级；主从架构；小概率丢失消息；延迟ms级；功能较极其完备。

​		综上：ActiveMQ功能最完备，但会小概率丢失消息，相比之下可靠性不高，社区不活跃；RabbitMQ延迟最低，主从架构，集群动态扩展稍麻烦，erlang语言开发，不利于研究和定制，基本职能依赖于开源社区的维护和修复，和ActiveMQ的吞吐量低RocketMQ和Kafaka一个量级；Kafka和RocketMQ的吞吐量都是10w级，延迟都是ms级，均为分布式架构，扩展方便，但Kafka功能较少，数据先写入磁盘缓冲区，未直接落盘，可能会丢失数据，适合大数据等适当丢失数据没关系、吞吐量要求高、不需要太多高级功能的场景；RocketMQ支持复杂的MQ业务场景，是java语言开发，可以定制。

#### 作用

​		MQ通过提供消息传递和消息排队模型，可以在分布式环境下提供应用解耦、异步通信、流量削峰、数据同步等功能，是分布式系统架构中的一个重要组件。缺点是引入MQ后，系统可用性降低，需要保证MQ的高可用，否则MQ挂了整套系统会崩溃；需要做额外的方案和架构来规避如消息重复消费、丢失、一致性、传递的顺序性等各种问题，导致系统的复杂度提高。

**解耦**：例如系统A跟其他系统严重耦合，很多系统需要系统A产生的某条关键数据，则A需要考虑是否调用成功、失败超时、是否重发等很多问题。引入MQ后，数据发到MQ里，需要该数据的系统直接去MQ消费，A不需关系其他问题。

**异步**：若一个请求需要写多个库，响应速度太慢，一般要求每个请求在200ms内完成，引入MQ，发送异步消息写库，可提升响应速度。

**削峰**：引入MQ后，将请求写入MQ，设置系统每秒从MQ拉取的请求不多于自己每秒能处理的最大请求数，则高峰期也不担心系统会挂掉，虽说高峰期会积压很多消息，但过后系统将快速的消费掉。

### 属性

#### Topic

​		主题，第一级消息类型，标识一类消息类型。

#### Tag

​		标签，第二级消息类型，对同一类型的消息进行细分。

### 角色

**Broker**：RocketMQ的进程称为Broker，集群中各个Broker接收不同的消息，然后存储在本地磁盘中。

**NameServer**：负责管理所有broker的信息，让生产者、消费者知道集群中有哪些Broker并与之通信。彼此之间相互独立，没有通信关系，全挂掉也不影响业务系统使用。没有频繁的读写，性能开销非常小，稳定性很高。

### 系统设计

1. MQ支持可伸缩性，需要时可快速扩容，增加吞吐量和容量→采用分布式设计系统
2. 数据防丢→持久化到磁盘→顺序写入→省去了磁盘随机读写的寻址开销，顺序读写性能高
3. 高可用→多副本，集群。

### 集群

#### 集群部署/工作流程

​		部署集群，将流量分散在多台机器上，以支撑高并发访问。我们采用broker主从架构+多副本策略来实现高可用。Master broker负责读和写，slave broker只负责读，这一个master和n个slave构成一个broker set。

1. Broker将自己的信息注册到集群中所有的NameServer上，主要信息有：
    1. clusterName：集群名字
    2. brokerAddr：ip:端口号
    3. brokerName：broker set的名字
    4. brokerId：唯一标识同个broke set中的各个broker（master为0，slave为正数）
    5. topicConfigWrapper：含order是否顺序消息+perm topic的权限+readQueueNums消费MQ数+writeQueueNums生产MQ数

2. NameServer便知道集群中有哪些broker，收发消息则会先去到NameServer获取路由信息，从而找到对应的broker。
3. 单个broker和所有nameserver保持长连接，每30s向NameServer发送心跳，NameServer每10s运行任务检查各个Broker最近一次心跳时间，若超过120s未发送心跳，则判定该Broker已挂掉。
4. 收发消息前，先创建 Topic 。创建 Topic 时，需要指定该 Topic 要存储在哪些 Broker上；也可以在发送消息时自动创建Topic。
5. Producer启动时，需指定NameServer的地址，从NameServer集群中随机选择一台建立长连接；producer每30s从NameServer中获取Topic跟broker的映射关系，更新到本地内存中；然后跟topic涉及的所有broker建立长连接，每30s发一次心跳。producer发送消息时，会自动轮询当前所有可用的broker，一条消息发送成功则下次换另一个broker发送，实现负载均衡。
6. master之间不交互，维护各自的commitLog和consumeQueue。CommitLog文件存储了所有元信息，包含消息体，即使Consume Queue中的数据丢失，仍可以根据commitlog恢复。
7. master收到消息后slave采取Pull模式拉取消息，master传输到slave的是CommitLog的物理文件，从而实现主从间的消息同步，这样一条消息就不止一份了，master宕机了还有slave中的可以用。要保证备用Broker与主用Broker信息是一致的，在备用Broker初始化时设置了定时任务，每个60秒调用SlaveSynchronize.syncAll()方法发起向主用Broker进行一次config类文件的同步，而消息数据的同步由主备Broker通过心跳检测的方式完成，每隔5秒进行一次心跳。
8. 消费者获取消息的请求首先会发到Master Broker上，Master会监控本身的QPS和Slave数据同步的情况，MasterBroker在返回消息的同时，会根据当时Master Broker的负载情况和Slave Broker的同步情况，向消费者建议下一次拉去消息的时候从Master还是Slave拉取。
9. Master Broker挂掉时slave仍可读。MQ4.5前：Slave Broker无法自动转成Master，需运维手动修改Slave的相关配置，重启机器将其调整为Master，将导致中间一段时间的不可用。MQ4.5+：支持Dledger机制，通过Dledger技术将某个Slave选作新Master对外提供服务，实现自动故障切换，只需要10s+。

#### 角色

##### master的角色

SYNC_MASTER：master每次将producer发送来的消息写入内存（磁盘）时会同步等待master将消息传输到slave。

ASYNC_MASTER：消息会异步复制到slave。

#### 集群部署模式比较

​		单master，重启或宕机，将导致整个服务不可用；所以配置多master，避免服务单点，但数据还是单点，此时若单台宕机，该机器上未被消费的消息在机器恢复之前不可被订阅，影响实时性，异步刷盘可能会丢失少量消息。在此基础上给每个master配多个slave，master宕机则仍可从slave消费，实现服务数据均无单点，若主从间用异步刷盘方式，master数据复制到slave期间若master磁盘损坏则会丢失少量消息，若为同步刷盘，性能约降低10%但可靠性提高。

注：当使用多master无slave的集群搭建方式时，master的brokerRole配置必须为ASYNC_MASTER。如果配置为SYNC_MASTER，则producer发送消息时，返回值的SendStatus会一直是SLAVE_NOT_AVAILABLE。

### 刷盘方式

#### 同步刷盘

​		数据到达内存后，同步到commitlog日志后，算同步刷盘成功，随后才返回producer数据已发送成功。

#### 异步刷盘

​		数据到达内存后，返回producer数据发送成功，之后再写入commitlog日志。异步复制性能约比同步复制低10%。

#### 配置

​		flushDiskType参数：SYNC_FLUSH或ASYNC_FLUSH

### 容错机制

​		Broker心跳和NameServer定时任务是周期性的，而生产者消费者拉取信息是实时性的，存在发送消息和消费消息失败的情况，对于生产者而言，有一套容错机制。

​		例如：若某个Broker没有宕机，只是与NameServer之间的网络延迟造成NameServer认为该Broker宕机了，Producer后序拿到新的路由信息，但此时Producer是可以连通原Broker的，但此时Producer并不会给该Broker发送消息，而是给新Broker发送消息。

### 发送/消费

#### 发送成功

​		做好集群部署和主从备份，提高可用性；给MQ配置消息持久化，防止MQ挂掉后消息丢失；做好监控，防止服务挂掉后没有检测到，及时重启；从业务角度保证MQ挂掉后的用户体验。

#### 顺序性

​		RocketMQ可以严格的保证消息有序，但这个顺序，不是全局顺序，是分区（queue）顺序，要全局顺序只能一个分区。消息发送默认是会采用轮询的方式发送到不通的queue（分区），而消费端消费会分配到多个queue，多个queue同时拉取提交消费。做业务逻辑时，Producer端保证发送消息有序，且发送到同一个队列。consumer端保证消费同一个队列。例如一个订单的顺序流程是：创建、付款、推送、完成，依赖订单ID进行hash，同样的ID进同一个队列，这样同一批你需要做到顺序消费的肯定会投递到同一个queue，同一个queue肯定会投递到同一个消费实例，同一个消费实例肯定是顺序拉取并顺序提交线程池的，只要保证消费端顺序消费。如果是使用MessageListenerOrderly则自带此实现，如果是使用MessageListenerConcurrently，则需要把线程池改为单线程模式。

#### 消费模式

​		消费模式分为推（push）模式和拉（pull）模式。推模式是指由 Broker 主动推送消息至消费端，实时性较好，不过需要一定的流制机制来确保服务端推送过来的消息不会压垮消费端。而拉模式是指消费端主动向 Broker 端请求拉取（一般是定时或者定量）消息，实时性较推模式差，但是可以根据自身的处理能力而控制拉取的消息量。

**consumer group**：消费同一类消息的多个 consumer 实例组成一个消费者组，也可以称为一个 consumer 集群，这些 consumer 实例使用同一个 group name，订阅的 tag 相同。

##### 集群消费

​		当 consumer 使用集群消费时，每条消息只会被 consumer 集群内的任意一个 consumer 实例消费一次。使用集群消费的时候，consumer 的消费进度是存储在 broker 上，consumer 自身是不存储消费进度的。消息进度存储在 broker 上的好处在于，当 consumer 集群扩大或者缩小时，由于消费进度统一在broker上，消息重复的概率会被大大降低了。在集群消费模式下，并不能保证每一次消息失败重投都投递到同一个 consumer 实例。

##### 广播消费

​		当 consumer 使用广播消费时，每条消息都会被 consumer 集群内所有的 consumer 实例消费一次，也就是说每条消息至少被每一个 consumer 实例消费一次。与集群消费不同的是，consumer 的消费进度是存储在各个 consumer 实例上，这就容易造成消息重复。还有很重要的一点，对于广播消费来说，是不会进行消费失败重投的，所以在 consumer 端消费逻辑处理时，需要额外关注消费失败的情况。虽然广播消费能保证集群内每个 consumer 实例都能消费消息，但是消费进度的维护、不具备消息重投的机制大大影响了实际的使用。因此，在实际使用中，更推荐使用集群消费，因为集群消费不仅拥有消费进度存储的可靠性，还具有消息重投的机制。而且我们通过集群消费也可以达到广播消费的效果。

**集群消费模式模拟广播消费模式**：可以通过创建多个 consumer 实例模拟广播消费，每个 consumer 实例属于不同的 consumer group，但它们都订阅同一个 topic。 每个 consumer 实例的消费逻辑可以一样也可以不一样，每个consumer group还可以根据需要增加 consumer 实例，比起广播消费来说更加灵活。

#### 重复消费

​		MQ不保证重复消费问题，需自行设计逻辑保证消息消费的幂等性。

#### 积压

​		消费端消费慢或不消费，消费时mysql挂了，消费端hang住不动，导致MQ积压大量message，消息队列集群的磁盘快写满了。

##### 排查

​		rocketmq自带的web控制台可看到broker的吞吐量，可以看到总消息数和消费的总偏移量，比较就知道积压了多少。在rocketMQ提供的web页面管理的war包rocketmq-tools-3.2.6.jar中找到了源码，提供了查看消费信息的方法，用getDiffNum()方法可获取到队列中的消息堆积量。rocketmq客户端有日志，记录消费进度、心跳等信息，支持log4j，在ClientLogger类里配置，也可以用try catch把错误打印在日志上排查。

##### 解决方案

​		发现问题后，采取紧急扩容。停掉现有consumer，写一个临时分发数据的consumer程序，部署上去消费积压的数据，直接轮询写入临时建立好的大量的queue，并不做耗时的处理。临时征用机器部署consumer，每一批consumer消费一个临时的queue中的数据。将queue资源和consumer资源扩大数倍，以正常的数倍速度来消费数据，称为紧急扩容。快速消费完积压的数据之后，恢复原先部署的架构，重新用原先的consumer机器消费消息。

#### 丢失

​		积压时间过长，消息到达过期时间而被清理，导致大量数据丢失。

##### 解决方案

​		解决消息因积压、过期、队列满等导致的过期被清理的方案：批量重导，手动写临时程序逐条查出丢失的数据，重新发到MQ中；扩容consumer和queue快速消费积压消息。

#### 回溯

​		消息在消费完成之后，可通过回溯消费复现“丢失的”消息进而查出问题的源头所在。消息回溯的作还有索引恢复、本地缓存重建，有些业务补偿方案也可以采用回溯的方式来实现。

##### 实现

​		push模式和pull模式都可以实现消息回溯。push模式不需要关心offset，只需要设置起始消费时间，pull模式要自己维护queue和offset。

1. 命令行程序和 nameserver 及 broker 交互
2. broker 查询 topic 分区，指定消息的时间，获得其offset，通知连接该 broker 的 consumer。
3. consumer 根据 broker 返回的 offsetTable 重置位点

#### 消费失败

​		RocketMQ提供了ack机制，以保证消息能够被正常消费。发送者为了保证消息肯定消费成功，只有使用方明确表示消费成功，RocketMQ才会认为消息消费成功。中途断电，抛出异常等都不会认为成功——即都会重新投递。

​		若使用了PullConsumer模式，如何ack、如何保证消费成功等均需要使用方自己实现，如消费时注入一个回调函数，业务实现消费回调的时候，当且仅当此回调函数返回成功，才算消费成功，否则回调消费失败的接口。

​		如果消费者消费不成功，RocketMQ会把消息丢入到重试队列中，重试机制反复投递直到投递成功，如果达到指定次数（默认16次）后还没有消费成功，则把消息投递到死信队列中。通过后台管理系统从死信队列中取出投递失败的消息，手动进行库存的扣减。

### 队列

#### 优先级队列

​		优先级队列不同于先进先出队列，优先级高的消息具备优先被消费的特权，这样可以为下游提供不同消息级别的保证。如果消费者的消费速度大于生产者的速度，并且消息中间件服务器Broker中没有消息堆积，那么对于发送的消息设置优先级也就没有什么实质性的意义了。

#### 延迟队列

​		当消息被发送以后，并不想让消费者立刻拿到消息，而是等待特定时间后，消费者才能拿到这个消息进行消费。

##### 基于消息的延迟

​		为每条消息设置不同的延迟时间，那么每当队列中有新消息进入的时候就会重新根据延迟时间排序，这会对性能造成极大的影响。

##### 基于队列的延迟

​		设置不同延迟级别的队列，比如 5s、10s、30s、1min、5mins、10mins 等，每个队列中消息的延迟时间都是相同的，这样免去了延迟排序所要承受的性能之苦，通过一定的扫描策略（比如定时）即可投递超时的消息。

#### **重试队列**

​		重试队列一般分成多个重试等级，每个重试等级一般也会设置重新投递延时，重试次数越多投递延时就越大。例如消息第一次消费失败入重试队列 Q1，Q1 的重新投递延迟为 5s，在 5s 过后重新投递该消息；如果消息再次消费失败则入重试队列 Q2，Q2 的重新投递延迟为 10s，在 10s 过后再次投递该消息。以此类推，重试越多次重新投递的时间就越久，为此需要设置一个上限，超过投递次数就入死信队列。

#### 死信队列

​		由于某些原因消息无法被正确的投递，为了确保消息不会被无故的丢弃，一般将其置于一个特殊角色的队列，这个队列一般称之为死信队列。

#### 回退队列

​		消费端消费消息失败时，为防止消息无故丢失而重新将消息回滚到 Broker 中。

### 事务

1. 事务发起方发送prepare消息到MQ server。这里的消息发送成功由自己保证。发送失败就直接取消操作别执行了。

2. 消息发送成功后，事务发起方执行本地事务。执行本地事务的过程中若执行端挂掉或超时，MQ会不停询问同组的其他producer来获取状态。

3. 事务发起方根据本地事务执行结果，给MQ server返回commit或rollback。

4. 若结果为rollback，MQ则删除该prepare消息不继续下发。

    若结果为commit，MQ则将该消息发送给consumer端。

    若MQ server未收到事务发起方返回的结果，则回查本地事务状态，回到步骤③。

5. consumer端消费成功机制由MQ保证，重试队列、死信队列。若进入死信队列则发送报警由人工来手工回滚和补偿。